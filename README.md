# Agente Aut√≥nomo de An√°lisis y S√≠ntesis de Documentaci√≥n T√©cnica
En este repositorio presenta un agente de IA capaz de recibir una URL de una documentaci√≥n t√©cnica (por ejemplo, la documentaci√≥n de una librer√≠a, un framework o una API), procesarla, entenderla y responder preguntas complejas sobre ella. El agente se expone a trav√©s de una API RESTful y su l√≥gica interna ha sido orquestada mediante LangGraph.

## Descripci√≥n General de la Soluci√≥n

## Arquitectura de la Soluci√≥n

### Arquitectura General
Documentation Agent es una soluci√≥n basada en microservicios que implementa un sistema de Retrieval-Augmented Generation (RAG) para consultas sobre documentaci√≥n. La arquitectura se basa en una aplicaci√≥n FastAPI con procesamiento as√≠ncrono usando Celery.

### Componentes de Infraestructura
La soluci√≥n est√° containerizada usando Docker Compose e incluye los siguientes servicios:

* **Qdrant:** Base de datos vectorial para almacenamiento de embeddings
* **PostgreSQL:** Base de datos relacional principal
* **RabbitMQ:** Message broker para Celery
* **Redis:** Backend de resultados para Celery
* **Worker:** Procesador de tareas as√≠ncronas
* **Flower:** Monitoreo de tareas Celery
<img width="359" height="341" alt="diagram1" src="https://github.com/user-attachments/assets/f91936f8-c6a3-4161-94d3-dc1c36b96b57" />

### API y Routers
La aplicaci√≥n expone dos routers principales:

1. **Process Router:** Maneja el procesamiento de documentaci√≥n process_router.py:10-21
2. **Chat Router:** Gestiona las consultas de chat y el historial
<img width="322" height="361" alt="diagram5" src="https://github.com/user-attachments/assets/d51a1ed8-1388-4145-ab3c-4cc5c976389e" />

### Pipeline de Procesamiento de Documentos
El procesamiento as√≠ncrono de documentos se realiza a trav√©s de una tarea Celery que:

1. Extrae contenido de URLs
2. Divide el contenido en chunks
3. Genera embeddings usando AWS Bedrock
4. Almacena los vectores en Qdrant
<img width="680" height="353" alt="diagram6" src="https://github.com/user-attachments/assets/c746c439-6890-42c3-bb0a-3395aca585de" />

### Flujo principal
<img width="363" height="654" alt="graph" src="https://github.com/user-attachments/assets/89f2125d-2d62-40b9-978d-8f63cfaf489c" />
El grafo principal de procesamiento de la consulta est√° construido en langgraph, uan tecnolog√≠a que permite construir workflow predecibles y flexibles. 
El diagrama muestra la arquitectura del grafo.

### Flujo RAG (Retrieval-Augmented Generation)
<img width="466" height="545" alt="rag-graph" src="https://github.com/user-attachments/assets/21eb956b-94a6-4571-b1c6-a2a058357680" />

La arquitectura implementa un subflujo RAG que deriva del flujo principal, usando LangGraph que incluye:

* **Retrieve:** B√∫squeda de documentos relevantes
* **Grade Documents:** Evaluaci√≥n de relevancia de documentos
* **Generate:** Generaci√≥n de respuestas
* **Transform Query:** Reescritura de consultas para mejorar resultados
El flujo incluye validaciones para detectar alucinaciones y verificar que las respuestas aborden correctamente las preguntas.  
La arquitectura del grafo implementa el patr√≥n Self-Reflective RAG, una t√©cnica de RAG en la que el modelo no solo genera una respuesta basada en informaci√≥n recuperada, sino que reflexiona sobre su propia respuesta para evaluar su validez y mejorarla si es necesario.
En lugar de limitarse a producir una respuesta directamente, el modelo:
1. Genera una primera respuesta usando los documentos recuperados.
2. Se autoeval√∫a para juzgar si la respuesta es correcta, relevante y bien fundamentada.
3. Corrige o ajusta la respuesta si encuentra errores, incoherencias o falta de soporte.
Este proceso permite respuestas m√°s precisas, fundamentadas y confiables, al reducir errores como alucinaciones o afirmaciones no respaldadas por la evidencia.
A continuaci√≥n se presenta el diagrama del flujo empleado como referencia:
<img width="1000" height="358" alt="reflective-2" src="https://github.com/user-attachments/assets/72f07400-7bda-49b9-ac37-9d898979e67d" />

### Servicios de IA
La soluci√≥n utiliza AWS Bedrock para:

* Generaci√≥n de embeddings de texto
* Rerank de fragmentos recuperados de la base de datos vectorial
* Consultas al modelo de lenguaje para generar respuestas

### Patr√≥n Arquitect√≥nico
La arquitectura sigue un patr√≥n de microservicios con:

* Separaci√≥n de responsabilidades (routers, controllers, services)
* Procesamiento as√≠ncrono para operaciones pesadas
* Base de datos vectorial especializada para b√∫squeda sem√°ntica
* Cache y gesti√≥n de estado distribuido
<img width="694" height="328" alt="diagram2" src="https://github.com/user-attachments/assets/179e29bf-ef95-4420-9867-14df7fcb9539" />

### Notas
Esta arquitectura est√° dise√±ada para manejar consultas sobre documentaci√≥n de manera escalable, usando tecnolog√≠as modernas de IA y procesamiento distribuido. El uso de LangGraph permite implementar flujos complejos de RAG con validaciones y mejoras autom√°ticas de consultas.

## Caracter√≠sticas y capacidades clave
| Caracter√≠stica                              | Descripci√≥n                                                                 | Implementaci√≥n                                                                 |
|---------------------------------------------|-----------------------------------------------------------------------------|--------------------------------------------------------------------------------|
| Procesamiento asincr√≥nico de documentos     | Ingiere y procesa documentos de fuentes web                                | Trabajadores de Celery con el agente de mensajes RabbitMQ                      |
| B√∫squeda y recuperaci√≥n de vectores         | B√∫squeda sem√°ntica a trav√©s de documentos procesados                       | Base de datos vectorial de Qdrant con incrustaciones de AWS Bedrock            |
| Interfaz de chat                            | Sesiones de chat interactivas con contexto de documentos                   | Puntos finales REST de FastAPI con persistencia de PostgreSQL                  |
| Respuestas impulsadas por RAG               | Respuestas contextuales utilizando documentos recuperados                  | Flujos de trabajo de LangGraph con LLM de Google GenAI                         |
| Estado del procesamiento en tiempo real     | Realizar un seguimiento del progreso del procesamiento de documentos       | Almacenamiento en cach√© de Redis con supervisi√≥n de tareas de Celery           |
| Integraci√≥n de IA multimodelo               | Integraci√≥n flexible de servicios de IA                                    | Abstracciones de servicios de AWS Bedrock y Google GenAI                       |


## Tecnolog√≠as Principales
<img width="754" height="350" alt="diagram3" src="https://github.com/user-attachments/assets/dcb6c03e-3444-4213-969b-b3d7101cf702" />  
El Documentation Agent utiliza una arquitectura de microservicios basada en tecnolog√≠as modernas para procesamiento de documentos y chat con IA.  

### Framework Web y API
* **FastAPI:** Framework web principal para la API REST
* **Pydantic:** Gesti√≥n de configuraci√≥n y validaci√≥n de datos
  
### Procesamiento As√≠ncrono
* **Celery:** Sistema de colas de tareas para procesamiento en segundo plano
* **RabbitMQ:** Message broker para las colas de Celery
* **Flower:** Monitoreo de workers de Celery
  
### Bases de Datos
* **PostgreSQL:** Base de datos relacional para historial de chats
* **Qdrant:** Base de datos vectorial para embeddings
* **Redis:** Cache y almacenamiento de resultados de tareas
  
### Servicios de IA
* **AWS Bedrock:** Embeddings y reranking de documentos
* **Google AI:** Servicios de inferencia LLM
* **Cohere:** Cliente para embeddings a trav√©s de AWS
  
### Procesamiento de Contenido
* **BeautifulSoup:** Extracci√≥n de contenido HTML
* **Requests:** Cliente HTTP para obtener documentaci√≥n web
  
### Infraestructura y Despliegue
* **Docker Compose:** Orquestaci√≥n de contenedores
* **SQLAlchemy:** ORM para PostgreSQL

El stack seleccionado es ampliamente utilizado en la industria para el desarrollo de aplicaciones con Inteligencia Artificial, ya que permite construir aplicaciones livianas robustas, mantenimiento un alto rendimiento en la ejecuci√≥n.
Se trata en general de tecnolog√≠as open-source con una amplia comunidad y abundante documentaci√≥n. A continuaci√≥n se precisan razones adicionales para la selecci√≥n del stack:

**FastAPI**
Es ideal para construir APIs modernas y asincr√≥nicas gracias a su rendimiento sobresaliente (basado en Starlette y Pydantic), su sintaxis intuitiva y su excelente soporte para documentaci√≥n autom√°tica con OpenAPI.

**Qdrant**
Es una base de datos vectorial optimizada para b√∫squedas sem√°nticas a gran escala, perfecta para almacenar y recuperar embeddings en flujos RAG como el que utiliza tu aplicaci√≥n.

**PostgreSQL**
Es una base de datos relacional robusta y altamente confiable, ideal para almacenar de forma estructurada el historial de conversaciones, metadatos y configuraciones del sistema.

**RabbitMQ**
Act√∫a como un message broker eficaz y maduro, facilitando la comunicaci√≥n as√≠ncrona entre componentes desacoplados como la API y los workers de procesamiento.

**Celery**
Permite ejecutar tareas pesadas o de larga duraci√≥n en segundo plano de forma distribuida y escalable, ideal para procesar documentos sin bloquear las solicitudes del usuario.

**Redis**
Es una base de datos en memoria ultrarr√°pida, ideal como backend de resultados de Celery y para gestionar cach√©s temporales que mejoran el rendimiento general de la aplicaci√≥n.

**Docker**
Permite empaquetar cada componente de la aplicaci√≥n con todas sus dependencias en contenedores aislados, lo que garantiza entornos reproducibles y facilita la portabilidad entre entornos de desarrollo, prueba y producci√≥n.

**Docker Compose**
Es ideal para orquestar m√∫ltiples contenedores (como la API, Qdrant, Redis, etc.) de manera declarativa y sencilla, lo que acelera el despliegue local y en entornos controlados con una sola instrucci√≥n.

**cohere.embed-multilingual-v3**
Es un modelo de generaci√≥n de embeddings multiling√ºe de alto rendimiento, que permite representar documentos y consultas en un espacio sem√°ntico compartido sin importar el idioma, lo que es crucial para construir un sistema de RAG accesible globalmente.

**cohere.rerank-v3-5:0**
Este modelo mejora la precisi√≥n de recuperaci√≥n reordenando los documentos m√°s relevantes seg√∫n su pertinencia con la consulta, lo que refina considerablemente la calidad de las respuestas generadas en el flujo RAG.

**LangGraph**
Es una librer√≠a dise√±ada para construir flujos conversacionales y pipelines de razonamiento estructurado como grafos, lo que permite orquestar de forma expl√≠cita y controlada procesos complejos como el RAG, con capacidad para incorporar validaciones, ramificaciones l√≥gicas e introspecci√≥n en tiempo de ejecuci√≥n.

**AWS Bedrock**
Proporciona acceso a modelos fundacionales l√≠deres (como Cohere, Anthropic, AI21) mediante una API totalmente gestionada, lo que permite integrar capacidades avanzadas de generaci√≥n y embeddings sin preocuparse por la infraestructura, escalabilidad o mantenimiento de modelos de IA.

## Estructura del Proyecto
```
üì¶ raiz_del_proyecto/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ celery_tasks/     # Tareas asincr√≥nicas (ej. con Celery)
‚îÇ   ‚îú‚îÄ‚îÄ config/           # Configuraci√≥n general del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ controllers/      # L√≥gica de negocio
‚îÇ   ‚îú‚îÄ‚îÄ db/               # Conexi√≥n y operaciones con base de datos
‚îÇ   ‚îú‚îÄ‚îÄ models/           # Modelos de datos (ORM o Pydantic)
‚îÇ   ‚îú‚îÄ‚îÄ routers/          # Definici√≥n de endpoints de la API
‚îÇ   ‚îú‚îÄ‚îÄ schemas/          # Esquemas Pydantic para validaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ services/         # Servicios auxiliares y utilitarios
‚îÇ   ‚îî‚îÄ‚îÄ workflows/        # Definici√≥n de grafos con LangGraph
‚îú‚îÄ‚îÄ requirements.txt      # Dependencias del proyecto
‚îú‚îÄ‚îÄ README.md             # Documentaci√≥n del proyecto
‚îî‚îÄ‚îÄ .env.example          # Variables de entorno de ejemplo
```
## Gu√≠a de Instalaci√≥n y Ejecuci√≥n
### Requisitos previos
* Docker
* Docker Compose >=2.0
* Credenciales de AWS Bedrock
### Paso a Paso
1. Clonar el repositorio:
```
git clone https://github.com/devmauriciopineda/documentation_agent.git
cd documentation_agent
```
2. Copiar el archivo de variables de entorno:
```
cp .env.example .env
```
3. Editar el archivo .env y completar con las credenciales y configuraci√≥n necesarias:
```
OPENAI_API_KEY=tu_clave
EMBEDDING_MODEL=text-embedding-3-small
# Agregar aqu√≠ otras variables necesarias para el proyecto
```
4. Levantar la aplicaci√≥n con Docker Compose:
```
docker-compose up --build
```
5. Acceder a la API desde el navegador o un cliente HTTP:
* Documentaci√≥n Swagger: http://localhost:8002/docs
### Variables de ambiente
En el archivo .env se deben especificar los valores de las variables de ambiente. A continuaci√≥n se presentan valores de referencia. Las credenciales para usar los modelos de Bedrock deben generarse utilizando el servicio IAM de AWS. Asimismo, para utilizar los modelos de gemini se requiere generar una API_KEY.
```
# AWS
AWS_ACCESS_KEY_ID='YOUR_ACCESS_KEY_ID'
AWS_SECRET_ACCESS_KEY='YOUR_SECRET_ACCESS_KEY'
AWS_REGION='us-east-1'
AWS_EMBEDDINGS_MODEL='cohere.embed-multilingual-v3'
AWS_RERANK_MODEL='cohere.rerank-v3-5:0'
AWS_RERANK_REGION='us-west-2'

# Google
GOOGLE_API_KEY='YOUR_GOOGLE_API_KEY'

# Inference
LLM_NAME='us.anthropic.claude-3-5-sonnet-20241022-v2:0'
TEMPERATURE=0.0
TOKENS=500

# Qdrant
COLLECTION_NAME='tech-docs'
QDRANT_URL='http://qdrant'

# RAG Config
MAX_CHUNKS_RETRIEVED=10
MAX_CHUNKS_RERANKED=3
DEFAULT_CHUNK_SIZE=1200
DEFAULT_CHUNK_OVERLAP=0

# Rabbit
RABBITMQ_DEFAULT_USER='YOUR_RABBIT_USER'
RABBITMQ_DEFAULT_PASS='YOUR_RABBIT_PASS'

# Postgres
POSTGRES_USER='YOUR_POSTGRES_USER'
POSTGRES_PASSWORD='YOUR_POSTGRES_PAA'
POSTGRES_DB='docs_agent'
POSTGRES_HOST='postgres'
POSTGRES_PORT='5432'
```
## Documentaci√≥n de la API
Una vez que el servidor est√© en funcionamiento, puedes acceder a la documentaci√≥n interactiva de la API proporcionada por FastAPI:
* Swagger UI: http://127.0.0.1:8002/docs
* ReDoc: http://127.0.0.1:8002/redoc
Estas interfaces permiten explorar y probar la API desde el navegador.

## Uso de la API
### POST /api/v1/process-documentation:
Recibe una URL de una documentaci√≥n y un chatId para iniciar el procesamiento as√≠ncrono. Devuelve una confirmaci√≥n inmediata y el estado del procesamiento (ej. "En progreso").

### GET /api/v1/processing-status/{chatId}:
Permite consultar el estado del procesamiento de la documentaci√≥n.

### POST /api/v1/chat/{chatId}:
Permite interactuar con el agente una vez que la documentaci√≥n ha sido procesada. Recibe una pregunta del usuario y devuelve una respuesta generada por el agente.

### GET /api/v1/chat-history/{chatId}:
Devuelve el historial de la conversaci√≥n.

Para los detalles el uso de la API, remitirse a la documentaci√≥n oficial en swagger: http://127.0.0.1:8002/docs
